\chapter{Tests on Discrete Data}

Data can be discrete for different reasons. On is that you acquired them in a discrete way (e.g. levels in a questionnaires.) Another one is that your paradigm only gives discrete results (e.g. rolling a dice). For the analysis of such data, we can build on the tools that we have already covered in the previous chapters.

\section{Comparing Groups of Ranked Data}

Ordinal data have clear rankings, e.g. "none - little - some - much - very much". However they are not continuous. For the analysis of such \emph{rank ordered data} we can use rank order methods for the analysis:

\begin{description}
  \item[Two groups] When comparing two rank ordered groups, we can use the \emph{Mann-Whitney test} \ref{test:Mann-Whitney}
  \item[Three or more groups]  When comparing two rank ordered groups, we can use the \emph{Kruskal-Wallis test} \ref{test:Kruskal-Wallis}

\end{description}

A more complex question arises when the requirement arises not only to compare two groups, but also to make quantitative predictions for ordinal data. For example, suppose you want to calculate the probability that a patient survives an operation, based on the amount of anesthetic he/she receives. The answer to this question involves statistical modelling, and the tool of \emph{logistic regression}\index{general}{logistic regression}. If more than two ordinal (i.e. naturally ranked) levels are involved, the so-called \emph{ordinal logistic regression}\index{general}{ordinal logistic regression} is used.

\emph{Hypothesis tests} allow you to state quantitative probabilities on the likelihood of a hypothesis. \emph{Linear Regression Modeling} allows you to make predictions and give confidence intervals for output variables that depend linearly on given inputs. But a large class of problems exceeds these requirements. For example, suppose you want to calculate the probability that a patient survives an operation, based on the amount of anesthetic he/she receives, and you want to find out how much anesthetic you can give the patient so that the chance of survival is at least 95\%.

To cover such questions \emph{Generalized Linear Models (GLMs)} have been introduced, which extend the technique of linear regression to a wide range of other problems. A general coverage of GLMs is beyond the goals of this book, and I would like to refer to the excellent book by Dobson \cite{Dobson2008}. While Dobson only gives solutions in \emph{R} and \emph{Stata}, I have made solutions in Python available for most of their problems (\url{https://github.com/thomas-haslwanter/dobson.git}).

In the following chapter I want to cover one commonly used case, \emph{logistic regression}, and its extension to \emph{ordinal logistic regression}. The Python solutions presented should allow the readers to solve similar problems on their own, and should give a brief insight in Generalized Linear Models.

\section{Logistic Regression}\index{general}{Logistic Regression}

So far we have been dealing with linear models, where a linear change on the input leads to a corresponding linear change on the output (Fig. \ref{fig:regression}):

 \begin{equation}\label{eq:linear}
   y = k*x + d + \epsilon
 \end{equation}

However, for many applications this model is not suitable. Suppose we want to calculate the probability that a patient survives an operation, based on the amount of anesthetic he/she receives. This probability is bounded on both ends, since it has to be a value between $0$ and $1$.

We can achieve such a bounded relationship, though, if we don't use the output of Eq. \ref{eq:linear} directly, but wrap it by another function:

\begin{equation}\label{eq:logisticFcn}
  p(x) = \frac{1}{ 1 + e^{ \;\beta x + \alpha } }
\end{equation}

In this model, the variable $\beta$ that describes how quickly the function changes from 1 to 0, and $\alpha$ indicates the location of this change. This function is used frequently, and is called the \emph{logistic function}\index{general}{logistic function}.

\subsection{Example: The Challenger Disaster}

On January 28, 1986, the twenty-fifth flight of the U.S. space shuttle program ended in disaster when one of the rocket boosters of the Shuttle Challenger exploded shortly after lift-off, killing all seven crew members. The presidential commission on the accident concluded that it was caused by the failure of an O-ring in a field joint on the rocket booster, and that this failure was due to a faulty design that made the O-ring unacceptably sensitive to a number of factors including outside temperature. Of the previous 24 flights, data were available on failures of O-rings on 23, (one was lost at sea), and these data were discussed on the evening preceding the Challenger launch, but unfortunately only the data corresponding to the 7 flights on which there was a damage incident were considered important and these were thought to show no obvious trend. The data are shown below:

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{../Images/Challenger_ORings.png}\\
  \caption{Failure of O-rings during space shuttle launches, as a function of temperature.}
\end{figure}

To simulate the probability of the O-rings failing, we can use the \emph{logistic function}:

\begin{equation*}
  p(t) = \frac{1}{ 1 + e^{ \;\beta t + \alpha } }
\end{equation*}

With a given p-value, the \emph{binomial distribution} (section \ref{sec:binomialDist}) determines the probability mass function for a given number of shuttle launches.

\lstinputlisting[label=py:Logistic,caption=logitShort.py, language=Python]{../Code3/logitShort.py}

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{../Images/ChallengerPlain.png}\\
  \caption{Probability for O-ring failure.}
  \label{fig:challengerPlain}
\end{figure}

\PyImg "logit.py" (p \pageref{py:logit}) shows the full code for Fig. \ref{fig:challengerPlain}.
\index{python}{modeling}

\vspace{5 mm}

To summarize, we have three elements in our model

\begin{enumerate}
  \item A probability distribution, which determines the probability of the outcome for a given trial (here the binomial distribution).
  \item A linear model that relates the co-variates (here the temperature) to the variates (the failure/success of an O-ring).
  \item A \emph{link-function}\index{general}{link-function} that wraps the linear model to produce the parameter for the probability distribution.
\end{enumerate}

%Similarly, a model that predicts a probability of making a yes/no choice (a Bernoulli variable) is even less suitable as a linear-response model, since probabilities are bounded on both ends (they must be between 0 and 1). Imagine, for example, a model that predicts the likelihood of a given person going to the beach as a function of temperature. A reasonable model might predict, for example, that a change in 10 degrees makes a person two times more or less likely to go to the beach. But what does "twice as likely" mean in terms of a probability? It cannot literally mean to double the probability value (e.g. 50% becomes 100%, 75% becomes 150%, etc.). Rather, it is the odds that are doubling: from 2:1 odds, to 4:1 odds, to 8:1 odds, etc. Such a model is a log-odds model.

\section{Generalized Linear Models}\index{general}{Generalized Linear Models}

The example above is an example of a \emph{Generalized Linear Models (GLMS)}, a powerful tool for the analysis of wide range of statistical models. Here I will only describe the general principles. For details I refer to the excellent book by \cite{Dobson2008}.

A GLM consists of three elements:

\begin{enumerate}
  \item A probability distribution from the \emph{exponential family}\index{general}{distributions!exponential family}.
  \item A linear predictor $\eta = X \cdot \beta$ .
  \item A link function $g$ such that $E(Y) = \mu = g^{-1}(\eta)$.
\end{enumerate}

\subsection{Exponential Family of Distributions}

The exponential family is a set of probability distributions of a certain form, specified below. This special form is chosen for mathematical convenience, on account of some useful algebraic properties, as well as for generality, as exponential families are in a sense very natural sets of distributions to consider. The exponential families include many of the most common distributions, including the normal, exponential, chi-squared, Bernoulli, Poisson distribution and many others. (A common distribution that is not from the exponential family is the T-distribution.)

In mathematical terms, an distribution from the exponential family has the general form

\begin{equation}\label{eq:exponentialFamily}
  f_X(x|\theta) = h(x) g(\theta) \exp \left ( \eta(\theta) \cdot T(x) \right )
\end{equation}

where $T(x), h(x), g(\theta), \eta(\theta), and A(\theta)$ are known functions.

\subsection{Linear Predictor and Link Function}

The linear predictor for GLM is the same as the one used for \emph{linear models}. The resulting terminology is unfortunately fairly confusing:

\begin{description}
  \item[General Linear Models] are models of the form $y=X\beta+ \epsilon$, where $\epsilon$ is normally distributed (see Chapter \ref{chapter:models}).
  \item[Generalized Linear Models] encompass a much wider class of models, including all distributions from the exponential family \emph{and} a link function.
\end{description}

The \emph{link function} is an arbitrary function, with the only requirements that it is continuous and invertable.

\section{Ordinal Logistic Regression}\index{general}{Logistic Ordinal Regression}

\footnote{This section has been taken with permission from Fabian Pedregosa's blog on ordinal logistic regression, http://fa.bianp.net/blog/2013/logistic-ordinal-regression/} The \emph{logistic ordinal regression} model, also known as the proportional odds was introduced in the early 80s by McCullagh \cite{McCullagh1980,McCullaghNelder1989} and is a generalized linear model specially tailored for the case of predicting ordinal variables, that is, variables that are discrete (as in classification) but which can be ordered (as in regression). It can be seen as an extension of the logistic regression model to the ordinal setting.

Given $X \in {\mathbb{R}^{n \times p}}$ input data and $y \in \mathbb{N}^n$ target values. For simplicity we assume $y$ is a non-decreasing vector, that is, ${y_1} \leq {y_2} \leq ...$ Just as the logistic regression models posterior probability $P(y=j|X_i)$ as the logistic function, in the logistic ordinal regression we model the \emph{cumulative} probability as the logistic function. That is,

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{../Images/ordinal_1.png}\\
  \includegraphics[width=0.4\textwidth]{../Images/ordinal_logistic.png}\\
  \caption{Toy example with three classes denoted in different colors. Also shown the vector of coefficients $w$ and the thresholds $\theta_0$ and $\theta_1$. (Figure from Fabian Pedregosa, with permission)}
\end{figure}

\begin{equation}
  P(y \leq j|X_i) = \phi(\theta_j - w^T X_i) = \frac{1}{1 + \exp(w^T X_i - \theta_j)}
\end{equation}

where $w,\theta$ are vectors to be estimated from the data and $\phi$ is the logistic function defined as $\phi(t)=\frac{1}{1+exp(−t)}$.

%Toy example with three classes denoted in different colors. Also shown the vector of coefficients w and the thresholds $\theta_0$ and $\theta_1$

Compared to multiclass logistic regression, we have added the constrain that the hyperplanes that separate the different classes are \emph{parallel} for all classes, that is, the vector $w$ is common across classes. To decide to which class will $X_i$ be predicted we make use of the vector of thresholds $\theta$. If there are $K$ different classes, $\theta$ is a non-decreasing vector (that is, ${\theta _1} \leq {\theta _2} \leq ... \leq {\theta _{K - 1}}$
 of size $K−1$. We will then assign the class $j$ if the prediction $w^T X$ (recall that it's a linear model) lies in the interval $[\theta_{j−1},\theta_j[$. In order to keep the same definition for extremal classes, we define $\theta _0 =  - \infty $ and $\theta_0 = + \infty$.

The intuition is that we are seeking a vector w such that $Xw$ produces a set of values that are well separated into the different classes by the different thresholds $\theta$. We choose a logistic function to model the probability $P(y \leq j|X_i)$ but other choices are possible. In the proportional hazards model \cite{McCullagh1980} the probability is modeled as $-\log(1 - P(y \leq j | X_i)) = \exp(\theta_j - w^T
X_i)$. Other link functions are possible, where the link function satisfies $\text{link}(P(y \leq j | X_i)) = \theta_j - w^T X_i$. Under this framework, the logistic ordinal regression model has a logistic link function and the proportional hazards model has a log-log link function.

The logistic ordinal regression model is also known as the proportional odds model, because the ratio of corresponding odds for two different samples $X_1$ and $X_2$ is $\exp(w^T(X_1 - X_2))$ and so does not depend on the class $j$ but only on the difference between the samples $X_1$ and $X_2$.

\subsection{Optimization}

Model estimation can be posed as an optimization problem. Here, we minimize the loss function for the model, defined as minus the log-likelihood:

\begin{equation}
  \mathcal{L}(w, \theta) = - \sum_{i=1}^n \log(\phi(\theta_{y_i} - w^T X_i) -  \phi(\theta_{y_i -1} - w^T X_i))
\end{equation}


In this sum all terms are convex on w, thus the loss function is convex over $w$. It might be also jointly convex over $w$ and $\theta$, although I haven't checked. I use the function \texttt{fmin\_slsqp} in \texttt{scipy.optimize} to optimize $\mathcal{L}$ under the constraint that $\theta$ is a non-decreasing vector. There might be better options, I don't know. If you do know, please leave a comment!.

Using the formula $\log(\phi(t))^\prime = (1 - \phi(t))$, we can compute the gradient of the loss function as


\begin{eqnarray*}
    \nabla_w \mathcal{L}(w, \theta) &=& \sum_{i=1}^n X_i (1 - \phi(\theta_{y_i} - w^T X_i) - \phi(\theta_{y_i-1} - w^T X_i)) \\
    \nabla_\theta \mathcal{L}(w, \theta) &=& \sum_{i=1}^n e_{y_i} \left(1 - \phi(\theta_{y_i} - w^T X_i) - \frac{1}{1 - \exp(\theta_{y_i -1} - \theta_{y_i})}\right)  \\
        \qquad  &+& e_{y_i -1}\left(1 - \phi(\theta_{y_i -1} - w^T X_i) - \frac{1}{1 - \exp(- (\theta_{y_i-1} - \theta_{y_i}))}\right)
\end{eqnarray*}

where $e_i$ is the $i$th canonical vector.

\subsection{Code}

I've implemented a Python version of this algorithm using Scipy's \lstinline{optimize.fmin_slsqp} function. This takes as arguments the loss function, the gradient denoted before and a function that is > 0 when the inequalities on θ are satisfied.

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{../Images/ordinal_bars.png}
  \caption{(Figure from Fabian Pedregosa, with permission)}
\end{figure}


Code can be found here as part of the minirank package, which is my sandbox for code related to ranking and ordinal regression. At some point I would like to submit it to scikit-learn but right now the I don't know how the code will scale to medium-scale problems, but I suspect not great. On top of that I'm not sure if there is a real demand of these models for scikit-learn and I don't want to bloat the package with unused features.
Performance

I compared the prediction accuracy of this model in the sense of mean absolute error (IPython notebook) on the boston house-prices dataset. To have an ordinal variable, I rounded the values to the closest integer, which gave me a problem of size 506 × 13 with 46 different target values. Although not a huge increase in accuracy, this model did give me better results on this particular dataset:

Here, ordinal logistic regression is the best-performing model, followed by a Linear Regression model and a One-versus-All Logistic regression model as implemented in scikit-learn.

\PyImg "ologit.py" (p \pageref{py:ologit}) corresponding Code by Fabian Pedregosa
\index{python}{ologit}
